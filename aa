from collections import Counter

import cupy as cp
import cv2
import numpy as np
from cuml import PCA, KMeans
from cuml.cluster import KMeans
from PIL import Image

from cuml.decomposition import IncrementalPCA


def perform_PCA(vector_set, n_components=25, batch_size=10000):
    """
    Perform PCA using Incremental PCA to handle large datasets.
    Args:
        vector_set (cupy.ndarray): The input data matrix.
        n_components (int): Number of principal components.
        batch_size (int): Size of data chunks for incremental fitting.
    Returns:
        cupy.ndarray: The principal components.
    """
    n_samples = vector_set.shape[0]
    ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)

    # Fit IncrementalPCA in batches
    for start in range(0, n_samples, batch_size):
        end = min(start + batch_size, n_samples)
        ipca.partial_fit(vector_set[start:end])

    # Get principal components
    EVS = cp.array(ipca.components_)
    return EVS


# def find_vector_set(diff_image, new_size):
#     if new_size[0] < 5 or new_size[1] < 5:
#         raise ValueError("Image size is too small for 5x5 blocks.")

#     # Use CuPy for GPU-accelerated operations
#     diff_image_gpu = cp.array(diff_image)
#     vector_set = cp.zeros((int(new_size[0] * new_size[1] / 25), 25), dtype=cp.float32)
#     i = 0

#     for j in range(0, new_size[0] - 4, 5):
#         for k in range(0, new_size[1] - 4, 5):
#             block = diff_image_gpu[j : j + 5, k : k + 5].ravel()
#             if block.shape[0] == 25:
#                 vector_set[i, :] = block
#                 i += 1

#     vector_set = vector_set[:i, :]
#     mean_vec = cp.mean(vector_set, axis=0)
#     vector_set -= mean_vec
#     return vector_set, mean_vec


def find_vector_set(diff_image, new_size, chunk_size=5000):
    if new_size[0] < 5 or new_size[1] < 5:
        raise ValueError("Image size is too small for 5x5 blocks.")

    # Allocate GPU memory lazily and use chunks
    diff_image_gpu = cp.array(diff_image)
    total_blocks = (new_size[0] - 4) * (new_size[1] - 4) // 25
    vector_set = cp.zeros((total_blocks, 25), dtype=cp.float32)

    i = 0
    block_count = 0
    for j in range(0, new_size[0] - 4, 5):
        for k in range(0, new_size[1] - 4, 5):
            block = diff_image_gpu[j : j + 5, k : k + 5].ravel()
            if block.shape[0] == 25:
                vector_set[block_count, :] = block
                block_count += 1
                if block_count >= chunk_size:  # Save memory by processing chunks
                    vector_set[i : i + chunk_size, :] = vector_set[:chunk_size]
                    block_count = 0
                    i += chunk_size

    vector_set = vector_set[: i + block_count, :]
    mean_vec = cp.mean(vector_set, axis=0)
    vector_set -= mean_vec

    return vector_set, mean_vec


def find_FVS(EVS, diff_image, mean_vec, new_size, chunk_size=10000):
    """
    Optimized `find_FVS` to minimize memory usage by processing in chunks.
    """
    diff_image_gpu = cp.array(diff_image)
    rows, cols = new_size[0] - 4, new_size[1] - 4
    total_blocks = rows * cols
    FVS = cp.zeros(
        (total_blocks, EVS.shape[0]), dtype=cp.float32
    )  # Pre-allocate memory

    block_idx = 0
    for start_row in range(2, rows + 2, chunk_size):
        end_row = min(start_row + chunk_size, rows + 2)
        feature_vector_set = []

        # Generate blocks within the chunk
        for i in range(start_row, end_row):
            for j in range(2, cols + 2):
                block = diff_image_gpu[i - 2 : i + 3, j - 2 : j + 3].ravel()
                feature_vector_set.append(block)

        # Convert chunk to GPU array and compute partial FVS
        feature_vector_set_gpu = cp.array(feature_vector_set, dtype=cp.float32)
        FVS_chunk = cp.dot(feature_vector_set_gpu, EVS.T) - mean_vec
        FVS[block_idx : block_idx + FVS_chunk.shape[0], :] = FVS_chunk

        # Update block index and free memory
        block_idx += FVS_chunk.shape[0]
        del feature_vector_set, feature_vector_set_gpu, FVS_chunk
        cp._default_memory_pool.free_all_blocks()

    return FVS


def clustering(FVS, components, new_size):
    """
    Optimized `clustering` to minimize memory usage during and after KMeans.
    """
    kmeans = KMeans(n_clusters=components)
    output = kmeans.fit_predict(FVS)  # Keeps the output on the GPU

    # Compute cluster counts directly on the GPU
    unique, counts = cp.unique(output, return_counts=True)
    least_index = unique[
        cp.argmin(counts)
    ].item()  # Find the cluster with the fewest points

    # Generate change map directly in sparse format
    change_map_gpu = cp.zeros((new_size[0] - 4, new_size[1] - 4), dtype=cp.uint8)
    change_map_gpu.ravel()[output == least_index] = 255

    return least_index, change_map_gpu.get()  # Convert to CPU only for final output


def find_PCAKmeans(imagepath1, imagepath2):
    # Load and preprocess images
    image1 = cv2.imread(imagepath1, cv2.IMREAD_GRAYSCALE).astype(np.float32)
    image2 = cv2.imread(imagepath2, cv2.IMREAD_GRAYSCALE).astype(np.float32)

    print("Images read")

    if image1.shape != image2.shape:
        raise ValueError("Input images must have the same dimensions.")

    new_size = (image1.shape[0] // 5 * 5, image1.shape[1] // 5 * 5)
    image1 = cv2.resize(image1, new_size)
    image2 = cv2.resize(image2, new_size)

    print("Images resized")

    diff_image = abs(image1 - image2)
    diff_image = (diff_image / diff_image.max() * 255).astype(np.uint8)

    vector_set, mean_vec = find_vector_set(diff_image, new_size)

    print("Vector set found")

    # Perform PCA with optimized memory usage
    EVS = perform_PCA(vector_set, n_components=25, batch_size=10000)

    print("PCA done")

    FVS = find_FVS(EVS, diff_image, mean_vec, new_size)
    components = 3

    print("FVS found")

    least_index, change_map = clustering(FVS, components, new_size)

    print("Clustering done")

    kernel = np.array(
        [
            [0, 0, 1, 0, 0],
            [0, 1, 1, 1, 0],
            [1, 1, 1, 1, 1],
            [0, 1, 1, 1, 0],
            [0, 0, 1, 0, 0],
        ],
        dtype=np.uint8,
    )
    cleanChangeMap = cv2.erode(change_map, kernel)

    Image.fromarray(change_map).save("changemap.jpg")
    Image.fromarray(cleanChangeMap).save("cleanchangemap.jpg")

    print("Images generated")


# import gc
# from collections import Counter
# from pathlib import Path

# import cv2
# import git
# import imageio.v2 as imageio
# import numpy as np
# from PIL import Image
# from sklearn.cluster import KMeans
# from sklearn.decomposition import PCA

# repo = git.Repo(".", search_parent_directories=True)
# OUTPUT_PATH = repo.working_tree_dir + "/out/"
# # Disable the safety check (use with caution!)
# Image.MAX_IMAGE_PIXELS = None
# # Or, set a higher limit
# # Image.MAX_IMAGE_PIXELS = 1000000000  # Example: Allow up to 1 billion pixels

# def find_vector_set(diff_image, new_size):
#     if new_size[0] < 5 or new_size[1] < 5:
#         raise ValueError("Image size is too small for 5x5 blocks.")

#     vector_set = np.zeros((int(new_size[0] * new_size[1] / 25), 25))
#     i = 0

#     for j in range(0, new_size[0] - 4, 5):  # Correct indexing
#         for k in range(0, new_size[1] - 4, 5):
#             block = diff_image[j : j + 5, k : k + 5]
#             feature = block.ravel()
#             if feature.shape[0] == 25:
#                 vector_set[i, :] = feature
#                 i += 1

#     if i == 0:  # Check if no blocks were added
#         raise ValueError("No valid 5x5 blocks found in the image.")

#     vector_set = vector_set[:i, :]  # Trim unused rows
#     mean_vec = np.mean(vector_set, axis=0)
#     vector_set = vector_set - mean_vec

#     return vector_set, mean_vec


# def find_FVS(EVS, diff_image, mean_vec, new):
#     i = 2
#     feature_vector_set = []

#     while i < new[0] - 2:
#         j = 2
#         while j < new[1] - 2:
#             block = diff_image[i - 2 : i + 3, j - 2 : j + 3]
#             feature = block.flatten()
#             feature_vector_set.append(feature)
#             j = j + 1
#         i = i + 1

#     FVS = np.dot(feature_vector_set, EVS)
#     FVS = FVS - mean_vec
#     print(f"\nFeature vector space size: {FVS.shape}")
#     return FVS


# def clustering(FVS, components, new):
#     kmeans = KMeans(components, verbose=0)
#     kmeans.fit(FVS)
#     output = kmeans.predict(FVS)
#     count = Counter(output)

#     least_index = min(count, key=count.get)

#     # Ensure reshape dimensions match the output size
#     expected_size = (new[0] - 4) * (new[1] - 4)
#     print(f"Output size: {output.size}, Expected size: {expected_size}")

#     if output.size != expected_size:
#         raise ValueError(
#             f"Cannot reshape: Output size {output.size} does not match the expected size {expected_size}"
#         )

#     # Reshape output correctly
#     change_map = np.reshape(output, (new[0] - 4, new[1] - 4))
#     return least_index, change_map


# def find_PCAKmeans(imagepath1, imagepath2):
#     # Load images
#     image1 = imageio.imread(imagepath1)
#     image2 = imageio.imread(imagepath2)

#     if image1.size == 0 or image2.size == 0:
#         raise ValueError("One or both images are empty.")
#     if image1.shape != image2.shape:
#         raise ValueError("Input images must have the same dimensions.")

#     # Resize images
#     new_size = np.asarray(image1.shape) // 5  # Integer division for resizing
#     new_size = new_size * 5  # Ensure size is a multiple of 5
#     image1 = cv2.resize(image1, (new_size[1], new_size[0])).astype(np.int16)
#     image2 = cv2.resize(image2, (new_size[1], new_size[0])).astype(np.int16)

#     # Compute absolute difference
#     diff_image = abs(image1 - image2)

#     # Convert diff_image to 8-bit format and save it
#     if diff_image.max() == 0:
#         diff_image_8bit = np.zeros(diff_image.shape, dtype=np.uint8)
#     else:
#         diff_image_8bit = (diff_image / diff_image.max() * 255).astype(np.uint8)

#     diff_image_pil = Image.fromarray(diff_image_8bit)
#     diff_image_pil.save("diff.jpg")

#     # PCA and K-means clustering
#     vector_set, mean_vec = find_vector_set(diff_image, new_size)
#     pca = PCA()
#     pca.fit(vector_set)
#     EVS = pca.components_

#     FVS = find_FVS(EVS, diff_image, mean_vec, new_size)
#     components = 3
#     least_index, change_map = clustering(FVS, components, new_size)

#     # Post-process change map
#     change_map[change_map == least_index] = 255
#     change_map[change_map != 255] = 0

#     change_map = change_map.astype(np.uint8)
#     kernel = np.asarray(
#         (
#             (0, 0, 1, 0, 0),
#             (0, 1, 1, 1, 0),
#             (1, 1, 1, 1, 1),
#             (0, 1, 1, 1, 0),
#             (0, 0, 1, 0, 0),
#         ),
#         dtype=np.uint8,
#     )
#     cleanChangeMap = cv2.erode(change_map, kernel)

#     # Save maps as images
#     Image.fromarray(change_map).save("changemap.jpg")
#     Image.fromarray(cleanChangeMap).save("cleanchangemap.jpg")


if __name__ == "__main__":
    q = "/home/korotole/repos/vesuvius-challenge-study/out/00-0.jpg"
    p = "/home/korotole/repos/vesuvius-challenge-study/out/01-0.jpg"
    # fname1, fname2 = preprocess_images(q, p)
    find_PCAKmeans(q, p)


# def preprocess_images(path1, path2):
#     filename1 = Path(path1).stem
#     filename2 = Path(path2).stem

#     img1 = cv2.imread(path1)
#     img2 = cv2.imread(path1)

#     if img1.shape != img2.shape:
#         print("Error: images of different shape")

#     h, w, _ = img1.shape
#     half = w // 2

#     left1 = img1[:, :half]
#     right1 = img1[:, half:]

#     left2 = img1[:, :half]
#     right2 = img1[:, half:]

#     cv2.imwrite(OUTPUT_PATH + filename1 + "-1.jpg", right1)
#     cv2.imwrite(OUTPUT_PATH + filename1 + "-0.jpg", left1)
#     cv2.imwrite(OUTPUT_PATH + filename2 + "-1.jpg", right2)
#     cv2.imwrite(OUTPUT_PATH + filename2 + "-0.jpg", left2)

#     return OUTPUT_PATH + filename1 + "-0.jpg", OUTPUT_PATH + filename2 + "-0.jpg"


from PIL import Image
import os
import re
import argparse
import pathlib

def main():
    """
    Parses command-line arguments and executes the image splitting function.
    """
    # Initialize the argument parser
    parser = argparse.ArgumentParser(description="Split an image into N equal parts and save them.")
    
    # Add arguments
    parser.add_argument("image_path", type=str, help="Path to the input image.")
    parser.add_argument("n", type=int, help="Number of equal parts to split the image into.")
    parser.add_argument("output_dir", type=str, help="Directory to save the split images.")
    
    # Parse the arguments
    args = parser.parse_args()
    
    # Call the image splitting function
    try:
        split_image_recursively(args.image_path, args.n, args.output_dir)
    except Exception as e:
        print(f"Error: {e}")


def split_image_recursively(image_path, n, output_dir):
    """
    Recursively splits an image into N parts, splitting along the largest dimension,
    and saves the parts in the specified output directory.

    Parameters:
        image_path (str): Path to the input image.
        n (int): The number of parts to split the image into.
        output_dir (str): Directory where the split images will be saved.

    Raises:
        ValueError: If the number of splits is not a power of 2 or n < 1.
    """
    # Validate that n is a power of 2
    if n < 1 or (n & (n - 1)) != 0:
        raise ValueError("The number of splits (N) must be a power of 2 and greater than 0.")
    
    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)

    def recursive_split(img, current_n, prefix):
        """Helper function to perform the recursive splitting."""
        if current_n == 1:
            # Save the current image part
            output_path = os.path.join(output_dir, f"{prefix}.png")
            img.save(output_path)
            return

        # Get dimensions and decide the splitting axis
        width, height = img.size        
        label1, label2 = '\0', '\0'
        if width >= height:
            split_axis = "vertical"
            mid_point = width // 2
            box1 = (0, 0, mid_point, height)  # Left part
            box2 = (mid_point, 0, width, height)  # Right part
            label1 = "00" # l
            label2 = "01" # r
        else:
            split_axis = "horizontal"
            mid_point = height // 2
            box1 = (0, 0, width, mid_point)  # Top part
            box2 = (0, mid_point, width, height)  # Bottom part
            label1 = "10" # t
            label2 = "11" # b

        # Split the image
        img1 = img.crop(box1)
        img2 = img.crop(box2)

        # Recursively split the resulting parts
        recursive_split(img1, current_n // 2, f"{prefix}-{label1}")
        recursive_split(img2, current_n // 2, f"{prefix}-{label2}")

    # Open the image and start recursive splitting
    img = Image.open(image_path)

    recursive_split(img, n, pathlib.Path(image_path).stem )
    print(f"Image successfully split into {n} parts and saved in '{output_dir}'.")

    """
            # List files in the given folder (non-recursive)
        for file in os.listdir(folder_path):
            if file.lower().endswith(tuple(allowed_extensions)):
                image_paths.append(os.path.join(folder_path, file))
    """


if __name__ == "__main__":
    main()
